Задание:
Сделайте добавление по умолчанию в любом контейнере параметра sysctl net.ipv4.conf.all.forwarding=1 (команды и вывод сохраните).
Запустите контейнер со следующими требованиями (команды и вывод сохраните):
дистрибутив - последний доступный Ubuntu LTS;
1 GiB RAM;
1 CPU;
отдельный диск на 5 GiB;
имя - rebrainme_lxc.
Установите в нем nginx, который будет отдавать строку Hello from LXC (команды и вывод сохраните).
Выведите список контейнеров с флагом -f и при помощи curl сделайте запрос с хоста к nginx, находящемуся в контейнере (команды и вывод сохраните).
При помощи nginx на хосте настройте проксирование запросов на порт 8090 в nginx, который находится в контейнере.
На проверку отправьте все сохраненные команды и выводы, адрес вашего сервера и конфигурационный файл с server для проксирования в контейнер.



****************************************************************************
Решение
****************************************************************************

Ефошкин Максим Вячеславович
ОТПРАВЛЕНО
13.11.2022 00:33
1. Установка
address: 130.193.55.161

apt update
sudo apt-get install lxc debootstrap bridge-utils lxc-templates cgroup-tools

root@epdp64kdjh3j0aoo5rau:/home/yc-user#  lxc-checkconfig
LXC version 4.0.12
Kernel configuration not found at /proc/config.gz; searching...
Kernel configuration found at /boot/config-5.4.0-124-generic
--- Namespaces ---
Namespaces: enabled
Utsname namespace: enabled
Ipc namespace: enabled
Pid namespace: enabled
User namespace: enabled
Network namespace: enabled

--- Control groups ---
Cgroups: enabled
Cgroup namespace: enabled

Cgroup v1 mount points: 
/sys/fs/cgroup/systemd
/sys/fs/cgroup/devices
/sys/fs/cgroup/hugetlb
/sys/fs/cgroup/pids
/sys/fs/cgroup/net_cls,net_prio
/sys/fs/cgroup/rdma
/sys/fs/cgroup/cpu,cpuacct
/sys/fs/cgroup/perf_event
/sys/fs/cgroup/cpuset
/sys/fs/cgroup/blkio
/sys/fs/cgroup/memory
/sys/fs/cgroup/freezer

Cgroup v2 mount points: 
/sys/fs/cgroup/unified

Cgroup v1 clone_children flag: enabled
Cgroup device: enabled
Cgroup sched: enabled
Cgroup cpu account: enabled
Cgroup memory controller: enabled
Cgroup cpuset: enabled

--- Misc ---
Veth pair device: enabled, not loaded
Macvlan: enabled, not loaded
Vlan: enabled, not loaded
Bridges: enabled, loaded
Advanced netfilter: enabled, not loaded
CONFIG_IP_NF_TARGET_MASQUERADE: enabled, not loaded
CONFIG_IP6_NF_TARGET_MASQUERADE: enabled, not loaded
CONFIG_NETFILTER_XT_TARGET_CHECKSUM: enabled, loaded
CONFIG_NETFILTER_XT_MATCH_COMMENT: enabled, not loaded
FUSE (for use with lxcfs): enabled, not loaded

--- Checkpoint/Restore ---
checkpoint restore: enabled
CONFIG_FHANDLE: enabled
CONFIG_EVENTFD: enabled
CONFIG_EPOLL: enabled
CONFIG_UNIX_DIAG: enabled
CONFIG_INET_DIAG: enabled
CONFIG_PACKET_DIAG: enabled
CONFIG_NETLINK_DIAG: enabled
File capabilities: 

Note : Before booting a new kernel, you can check its configuration
usage : CONFIG=/path/to/config /usr/bin/lxc-checkconfig

# Смонтированные точки
root@epdp64kdjh3j0aoo5rau:/home/yc-user# mount
sysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)
proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
udev on /dev type devtmpfs (rw,nosuid,noexec,relatime,size=985712k,nr_inodes=246428,mode=755)
devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)
tmpfs on /run type tmpfs (rw,nosuid,nodev,noexec,relatime,size=203068k,mode=755)
/dev/vda2 on / type ext4 (rw,relatime,errors=remount-ro)
securityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)
tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)
tmpfs on /run/lock type tmpfs (rw,nosuid,nodev,noexec,relatime,size=5120k)
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime)
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)
pstore on /sys/fs/pstore type pstore (rw,nosuid,nodev,noexec,relatime)
none on /sys/fs/bpf type bpf (rw,nosuid,nodev,noexec,relatime,mode=700)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset,clone_children)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
systemd-1 on /proc/sys/fs/binfmt_misc type autofs (rw,relatime,fd=28,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=13963)
hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime,pagesize=2M)
mqueue on /dev/mqueue type mqueue (rw,nosuid,nodev,noexec,relatime)
tracefs on /sys/kernel/tracing type tracefs (rw,nosuid,nodev,noexec,relatime)
debugfs on /sys/kernel/debug type debugfs (rw,nosuid,nodev,noexec,relatime)
fusectl on /sys/fs/fuse/connections type fusectl (rw,nosuid,nodev,noexec,relatime)
configfs on /sys/kernel/config type configfs (rw,nosuid,nodev,noexec,relatime)
tmpfs on /run/user/1000 type tmpfs (rw,nosuid,nodev,relatime,size=203064k,mode=700,uid=1000,gid=1001)
lxcfs on /var/lib/lxcfs type fuse.lxcfs (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)
root@epdp64kdjh3j0aoo5rau:/home/yc-user# 

2. Сделайте добавление по умолчанию в любом контейнере параметра sysctl net.ipv4.conf.all.forwarding=1 (команды и вывод сохраните).
root@epdp64kdjh3j0aoo5rau:/home/yc-user# cat /etc/lxc/default.conf 
lxc.net.0.type = veth
lxc.net.0.link = lxcbr0
lxc.net.0.flags = up
lxc.net.0.hwaddr = 00:16:3e:xx:xx:xx
net.ipv4.conf.all.forwarding=1

3. Запустите контейнер со следующими требованиями (команды и вывод сохраните):
дистрибутив - последний доступный Ubuntu LTS; 1 GiB RAM; 1 CPU; отдельный диск на 5 GiB; имя - rebrainme_lxc.




fdisk /dv/sdb
mkfs.xfs /dev/sdb1
vi /etc/fstab
/dev/sdb1   /var/lib/lxc/rebrainme_lxc xfs defaults 0 0
mkdir /var/lib/lxc/rebrainme_lxc
mount -a

lxc-create -t download -n rebrainme_lxc -- --dist ubuntu --release jammy --arch amd64
lxc-ls -f
NAME          STATE   AUTOSTART GROUPS IPV4 IPV6 UNPRIVILEGED 
rebrainme_lxc STOPPED 0         -      -    -    false 


df -h | grep rebrainme
/dev/sdb1                          5.0G   68M  5.0G   2% /var/lib/lxc/rebrainme_lxc

vi /var/lib/lxc/rebrainme_lxc/config 
lxc.cgroup.cpuset.cpus = 0
lxc.cgroup.memory.limit_in_bytes = 1G

root@ubuntu-server:/var/lib/lxc/rebrainme_lxc# cat /var/lib/lxc/rebrainme_lxc/config 
# Template used to create this container: /usr/share/lxc/templates/lxc-download
# Parameters passed to the template: --dist ubuntu --release jammy --arch amd64
# For additional config options, please look at lxc.container.conf(5)

# Uncomment the following line to support nesting containers:
#lxc.include = /usr/share/lxc/config/nesting.conf
# (Be aware this has security implications)

net.ipv4.conf.all.forwarding=1

# Distribution configuration
lxc.include = /usr/share/lxc/config/common.conf
lxc.arch = linux64

# Container specific configuration
lxc.rootfs.path = dir:/var/lib/lxc/rebrainme_lxc/rootfs
lxc.uts.name = rebrainme_lxc

# Network configuration
lxc.net.0.type = veth
lxc.net.0.link = lxcbr0
lxc.net.0.flags = up
lxc.net.0.hwaddr = 00:16:3e:20:83:a6
lxc.cgroup.cpuset.cpus = 0
lxc.cgroup.memory.limit_in_bytes = 1G

4. Установите в нем nginx, который будет отдавать строку Hello from LXC (команды и вывод сохраните).
lxc-start -n rebrainme_lxc
lxc-attach -n rebrainme_lxc
apt install nginx

rm /var/www/html/index.nginx-debian.html
vi /var/www/html/hello
Hello from LXC
vi /etc/nginx/sites-enabled/default
index hello;
systemctl enable --now nginx

root@rebrainmelxc:/var/www# curl localhost
Hello from LXC


5. Выведите список контейнеров с флагом -f и при помощи curl сделайте запрос с хоста к nginx, находящемуся в контейнере (команды и вывод сохраните).
root@ubuntu-server:/var/lib/lxc/rebrainme_lxc# lxc-ls -f
NAME          STATE   AUTOSTART GROUPS IPV4       IPV6 UNPRIVILEGED 
rebrainme_lxc RUNNING 0         -      10.0.3.111 -    false        

root@ubuntu-server:/var/lib/lxc/rebrainme_lxc# curl 10.0.3.111
Hello from LXC


6. При помощи nginx на хосте настройте проксирование запросов на порт 8090 в nginx, который находится в контейнере.
apt install nginx
 vi /etc/nginx/sites-enabled/default 
 location / {
 proxy_pass http://10.0.3.111;
 }
 
root@ubuntu-server:/var/lib/lxc/rebrainme_lxc# curl localhost
Hello from LXC


7. На проверку отправьте все сохраненные команды и выводы, адрес вашего сервера и конфигурационный файл с server для проксирования в контейнер.
В общем, сначала я поднял виртуалку в облаке, потом не смог добавить доп жесткий диск в облаке. Решил создать локально виртуалку.
Все делалось на виртуалке локальной. 
Локально все. 
В облаке сделаю прокси пасс тогда.

root@ubuntu-server:/var/lib/lxc/rebrainme_lxc# cat /etc/nginx/sites-enabled/default | grep -v -E '^#|^$|^\s*#'
server {
	listen 80 default_server;
	listen [::]:80 default_server;
	root /var/www/html;
	index index.html index.htm index.nginx-debian.html;
	server_name _;
	location / {
		proxy_pass http://10.0.3.111;
		try_files $uri $uri/ =404;
	}
}


Для проверки сделал и в облачно
curl http://130.193.55.161


[m.efoshkin@fedora ~]$ curl 130.193.55.161
Hello from LXC


ОТВЕТ КУРАТОРА
Алексей Кузнецов (@Hystrix)
ВЫПОЛНЕНО 5
14.11.2022 10:19
Добрый день!

"В общем, сначала я поднял виртуалку в облаке, потом не смог добавить доп жесткий диск в облаке. Решил создать локально виртуалку" - в принципе такая возможность в YC есть, но здесь можно было создать диск на основе файла:

# truncate -s 5G /var/lib/lxc/rebrainme_lxc.img
# mkfs.ext4 -F /var/lib/lxc/rebrainme_lxc.img
# mkdir /var/lib/lxc/rebrainme_lxc
# mount -t ext4 -o loop /var/lib/lxc/rebrainme_lxc.img /var/lib/lxc/rebrainme_lxc
# echo "/var/lib/lxc/rebrainme_lxc.img /var/lib/lxc/rebrainme_lxc ext4 loop 0 0" >> /etc/fstab
Если интересно, почитайте про loop-устройства. Иногда у нас бывает образ диска в файле и его нужно смонтировать.

Отличные знание темы и выполнение задания, хорошо что применяете бизнес-подход, т.е. сперва решение задачи/проблемы, потом уже детальное разбирательство (технический долг), замечаний и вопросов нет!


*****************************************************************************
Теория
*****************************************************************************


LNXA-08 03: Infrastructure. LXC
Описание:
Вопрос разделения ресурсов вычислительной техники стоит остро уже давно и если ранее, когда компьютер занимал целый дом и выполнял задачи в конкретной среде исполнения, он сводился скорее к разделению рабочего времени, который решали через обычный журнал с расписанием, когда кому предоставить возможность производить вычисления. Сейчас компьютер размером с ладонь способен обрабатывать десятки, а то и сотни задач практически одновременно, поэтому вопрос разделения времени практически не актуален. Однако из-за минимизации вычислительной техники вкупе с увеличением мощностей сейчас часто встречается вопрос разделения другого ресурса, а точнее ресурсов - вычислительных (дисковое пространство, оперативная память и процессорное время). В рамках следующих заданий мы рассмотрим с вами некоторые, хотя и не все, средства, которые позволяют решить данный вопрос.

Перед тем, как описывать существующие решения, стоит предварительно разобраться, какие технологии позволяют реализовать данную задачу. Пока мы приведем этот список кратко, далее - будем их расширять в последующих заданиях:

Виртуализация - позволяет запускать на одной физической вычислительной единице (так называемая хост-машина) несколько виртуальных (виртуальных машин). Эти машины обладают своими ограниченными ресурсами, которые выделяются из имеющегося объема ресурсов физической машины. Данный метод позволяет запускать практически любую операционную систему, с которой можно работать, так, если бы это была совершенно отдельная физическая машина.
Паравиртуализация - подвид виртуализации, при котором запускаемая операционная система знает, что она запущена не на железном сервере, а в роли виртуальной машины. Требует вносить изменения в запускаемую операционную систему для корректной отправки запросов к хосту на выполнение конкретных команд, использования ресурсов или запросов ввода-вывода (диск, сеть). Сразу скажем, что этот вид виртуализации используется нечасто, поэтому в рамках практикума мы не будем его рассматривать.
Контейнеризация операционной системы - подразумевает, что и хост, и виртуальная машина используют одно ядро операционной системы (что накладывает ограничение на то, какие операционные системы можно запустить), однако имеет свою файловую систему, сетевое пространство и свое пространство процессов, что реализуется за счет операционной системы хоста.
Контейнеризация приложения - вид контейнеризации, которой используется для запуска конкретного приложения со всеми нужными ему зависимостями, без необходимости запускать целую операционную систему.
В рамках этого задания мы рассмотрим одного из представителей контейнеризации операционной системы, доступной только в Linux, - Linux Containers или кратко LXC. Этот механизм позволяет исполнять множество изолированных Linux-систем (контейнеров) в одной системе. Контейнеры при этом будут использовать то же ядро, что доступно и у хоста, однако вы можете изолировать конкретные ресурсы, как и в случае виртуализации.

Важным отличием контейнеризации является то, что для работы ОС в контейнере требуется намного меньше ресурсов, чем в случае с виртуализацией. БОльшая часть работы возлагается на уже используемое ядро, а все общение с внешним миром производится через интерфейсы хоста - с точки зрения хоста все, что творится внутри LXC контейнера, - это просто очередной процесс со своим ветвлением и ресурсами. Внутри контейнера же создается видимость, что система абсолютно независима и не пересекается с хостовой.

LXC использует следующие возможности Linux:

namespaces - технология, позволяющая изолировать:
сеть;
процесс;
межпроцессорное взаимодействие;
файловые системы;
ядро ОС;
cgroups - позволяет ограничивать контейнер по ресурсам (RAM, CPU, I/O).
Разберем основные конфигурационные файлы, связанные с LXC:

# cat /etc/lxc/lxc-usernet # этот файл разрешает конкретному пользователю системы создавать виртуальные машины и подключать их к конкретному bridge.
# USERNAME TYPE BRIDGE COUNT
# username veth lxcbr0 10 # позволит пользователю _username_ подключать к bridge _lxcbr0_ до 10 контейнеров.

# cat /etc/lxc/default.conf # конфигурационныый файл, который используется по умолчанию при создании новых контейнеров.
lxc.net.0.type = veth
lxc.net.0.link = lxcbr0
lxc.net.0.flags = up
lxc.net.0.hwaddr = 00:16:3e:xx:xx:xx

# cat /etc/default/lxc # данный файл содержит переменные окружения, которые устанавливаются для работы LXC.
# LXC_AUTO - whether or not to start containers at boot
LXC_AUTO="true"

# BOOTGROUPS - What groups should start on bootup?
#	Comma separated list of groups.
#	Leading comma, trailing comma or embedded double
#	comma indicates when the NULL group should be run.
# Example (default): boot the onboot group first then the NULL group
BOOTGROUPS="onboot,"

# SHUTDOWNDELAY - Wait time for a container to shut down.
#	Container shutdown can result in lengthy system
#	shutdown times.  Even 5 seconds per container can be
#	too long.
SHUTDOWNDELAY=5

# OPTIONS can be used for anything else.
#	If you want to boot everything then
#	options can be "-a" or "-a -A".
OPTIONS=

# STOPOPTS are stop options.  The can be used for anything else to stop.
#	If you want to kill containers fast, use -k
STOPOPTS="-a -A -s"

USE_LXC_BRIDGE="false"  # overridden in lxc-net

[ ! -f /etc/default/lxc-net ] || . /etc/default/lxc-net
Как видно, у LXC есть большое количество точек входа для конфигурации еще даже без контейнеров - далее у нас будут появляться только новые файлы, связанные с новыми контейнерами. Документацию по флагам конфигурации LXC можно почитать на man странице lxc.container.conf.

После установки LXC использует bridge с именем по умолчанию lxcbr0, к которому будут подключаться все запускаемые контейнеры при помощи динамически создаваемых virtual ethernet устройств. lxcbr0 создается по умолчанию после установки, поэтому менять это не требуется.

Разбор дальнейших возможностей LXC мы начнем рассматривать уже на конкретных примерах применения командных утилит.

lxc-create
Данная команда позводяет запустить новый контейнер. Общая схема запуска:

lxc-create --name=NAME --template=TEMPLATE [OPTION...]

Если с первым параметром (именем) все понятно, то со вторым требуется объяснение - LXC контейнеры запускаются из шаблонов (templates). Они представляют из себя сформированный особым методом архив файлов операционной системы, из которых и создается контейнер. Список доступных для скачивания шаблонов есть на официальном сайте проекта LXC, однако это не мешает вам создать собственный шаблон при необходимости.

Для начала мы рекомендуем вам использовать в роли параметра template значение download - оно позволит выбрать необходимый для скачивания шаблон, который далее будет доступен в вашей локальной файловой системе.

lxc-ls
Позволяет вывести список имеющихся контейнеров. Обладает рядом параметров, которые позволяют узнать больше информации о состоянии контейнеров. К примеру, параметр -f позволят выводить не только имя контейнера, но и статус:

# lxc-ls -f
NAME STATE   AUTOSTART GROUPS IPV4 IPV6 UNPRIVILEGED
C1   STOPPED 0         -      -    -    false
В выводе выше видно, что у нас есть контейнер с именем C1, который на данный момент остановлен.

Пока он в таком состоянии, рассмотрим его конфигурационный файл:

# ls /var/lib/lxc/C1/ -1 # файлы, связанные со всеми конфигурационными файлами, хранятся в директории /var/lib/lxc с поддиректориями под каждый контейнер.
config # в этом файле хранится конфигурация контейнера.
rootfs # здесь хранится корневая система контейнера, созданная из шаблона.
root@analytics-dev:~# cat /var/lib/lxc/C1/config
# Template used to create this container: /usr/share/lxc/templates/lxc-download
# Parameters passed to the template:
# Template script checksum (SHA-1): 273c51343604eb85f7e294c8da0a5eb769d648f3
# For additional config options, please look at lxc.container.conf(5)

# Uncomment the following line to support nesting containers:
#lxc.include = /usr/share/lxc/config/nesting.conf
# (Be aware this has security implications)


# Distribution configuration
lxc.include = /usr/share/lxc/config/common.conf # стоит обратить внимание на этот файл - он содержит параметры конфигурации, которые подключаются ко всем контейнерам.
lxc.arch = linux64

# Container specific configuration
lxc.rootfs.path = dir:/var/lib/lxc/C1/rootfs
lxc.uts.name = C1

# Network configuration # эти параметры вы уже видели ранее в файле /etc/lxc/default.conf.
lxc.net.0.type = veth
lxc.net.0.link = lxcbr0
lxc.net.0.flags = up
lxc.net.0.hwaddr = 00:16:3e:a0:0b:e9

# cat  /var/lib/lxc/C1/rootfs/etc/*release # убедимся, что мы развернули контейнер Alpine Linux
3.12.0
NAME="Alpine Linux"
ID=alpine
VERSION_ID=3.12.0
PRETTY_NAME="Alpine Linux v3.12"
HOME_URL="https://alpinelinux.org/"
BUG_REPORT_URL="https://bugs.alpinelinux.org/"
lxc-start/lxc-stop
Данные команды позволяют запустить/остановить контейнер, соответственно. Так, для запуска выведенного выше контейнера C1 требуется ввести команду:

lxc-start C1

В работе контейнера можно убедиться по выводу lxc-ls:

NAME STATE   AUTOSTART GROUPS IPV4      IPV6 UNPRIVILEGED
C1   RUNNING 0         -      10.0.3.78 -    false
Как видно, в выводе поменялось состояние и у контейнера появился IP-адрес, по которому он доступен с нашего хоста.

lxc-top
Как и команда top, позволяет вывести используемые ресурсы контейнеров. Вывод данной команды обновляется каждые 3 секунды по умолчанию, но может меняться при помощи параметров запуска. Пример вывода:

Container                   CPU          CPU          CPU                                BlkIO        Mem       KMem
Name                       Used          Sys         User                    Total(Read/Write)       Used       Used
C1                         0.25         0.13         0.09        60.00 KiB(56.00 KiB/4.00 KiB)   2.29 MiB   1.71 MiB
TOTAL 1 of 1               0.25         0.13         0.09        60.00 KiB(56.00 KiB/4.00 KiB)   2.29 MiB   1.71 MiB
lxc-info
Позволяет выводить всю информацию о контейнере, включая информцию из lxc-ls и lxc-top:

# lxc-info -n C1
Name:           C1
State:          RUNNING
PID:            3937
IP:             10.0.3.78
CPU use:        0.25 seconds
BlkIO use:      60.00 KiB
Memory use:     2.29 MiB
KMem use:       1.71 MiB
Link:           vethF0QSDA
 TX bytes:      2.19 KiB
 RX bytes:      3.27 KiB
lxc-attach
Позволяет войти в контейнер для выполнения необходимых команд:

host ~# lxc-attach -n C1
~ #
~ #
~ # ls
~ # cat /etc/*release
3.12.0
NAME="Alpine Linux"
ID=alpine
VERSION_ID=3.12.0
PRETTY_NAME="Alpine Linux v3.12"
HOME_URL="https://alpinelinux.org/"
BUG_REPORT_URL="https://bugs.alpinelinux.org/"
На этом знакомство с базовыми командами, которые необходимы для работы с LXC, закончено, но перед началом практической работы требуется разобрать еще одну важную тему - ограничение ресурсов.

Ограничение вычислительных ресурсов
Позволяет при помощи cgroups ограничить использование ряда ресурсов при помощи директив конфигурационного файла семейства lxc.cgroup, где после cgroup указывается параметр, который требуется установить. К примеру, для выделения только одного ядра для процессора нужно использовать директиву lxc.cgroup.cpuset.cpus = 0, где 0 - идентификатор первого ядра.

Информацию о всех возможных параметрах можно найти в документации ядра Linux.

Ограничение хранилища
По умолчанию LXC разархивирует систему контейнера в файловую систему хоста, однако, если требуется ограничить объем доступного места в контейнере, можно использовать один из следующих способов:

Использовать дополнительный диск - для этого нужно перед созданием контейнера предварительно примонтировать диск в директорию, в которой будут храниться файлы контейнера.
Использовать внешнее решение для хранения - этот метод поддерживается в lxc-create и поддерживает работу с LVM, Ceph и ZFS, но в рамках этого задания рассматриваться не будет.
На этом знакомство с LXC можно было бы закончить, но необходимо еще упомянуть про такой инструмент, как LXD, который был создан компанией Canonical (создатели Ubuntu). Данный проект базируется на LXC, но расширяет его возможности (к примеру, привносит возможности живой миграции, вводит понятие образов вместо шаблонов, а таже берет на себя вопросы управления конфигурацией, хранилищем, ограничением ресурсов). Путаницу использования этого инструмента и чистого LXC вносит используемая для LXD единая CLI утилита с именем lxc, в то время как в LXC используются команды семейства lxc-*.

Полезные ссылки:
Para virtualization vs Full virtualization vs Hardware assisted Virtualization
Что такое LXC?
LXC (xgu.ru)
LXC (ubuntu rus docs)
Туториал по контейнеризации при помощи LXC
Использование контейнеров LXC в Debian/Ubuntu (opennet)
Linux-контейнеры дома: зачем и как (habr)
Что такое LXD?
LXC and LXD: Explaining Linux Containers
